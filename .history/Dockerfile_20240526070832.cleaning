# Start with the official Airflow image for version 2.9.1
FROM apache/airflow:2.9.1

# Set the working directory
WORKDIR /opt/airflow

# Install any necessary dependencies
RUN pip install --no-cache-dir \
    requests
    # Include any other dependencies needed for the data cleaning
    # Example: pandas, nltk

# Copy the data cleaning DAG file into the DAGs directory
COPY /home/moraa/Documents/10_academy/week4/scalable_data_warehouse/dags/data_processing.py /opt/airflow/dags/

# Copy other necessary files (e.g., the api controllers and scripts) if needed
COPY /home/moraa/Documents/10_academy/week4/scalable_data_warehouse/api/controllers /opt/airflow/api/controllers
COPY /home/moraa/Documents/10_academy/week4/scalable_data_warehouse/scripts/clean_data.py /opt/airflow/scripts/

# Set environment variables (optional, for Airflow configuration)
ENV AIRFLOW__CORE__EXECUTOR=LocalExecutor
ENV AIRFLOW__CORE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db

# Command to start the Airflow scheduler and web server
CMD ["airflow", "standalone"]
